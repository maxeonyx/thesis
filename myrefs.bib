


@article{manipnet,
  title={ManipNet: Neural manipulation synthesis with a hand-object spatial representation},
  author={Zhang, He and Ye, Yuting and Shiratori, Takaaki and Komura, Taku},
  journal={ACM Transactions on Graphics (ToG)},
  volume={40},
  number={4},
  pages={1--14},
  year={2021},
  publisher={ACM New York, NY, USA},
  url={https://www.ipab.inf.ed.ac.uk/cgvu/zhang2021.pdf}
}

@article{ViT,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{sim-mim,
  author    = {Zhenda Xie and
               Zheng Zhang and
               Yue Cao and
               Yutong Lin and
               Jianmin Bao and
               Zhuliang Yao and
               Qi Dai and
               Han Hu},
  title     = {SimMIM: {A} Simple Framework for Masked Image Modeling},
  journal   = {CoRR},
  volume    = {abs/2111.09886},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.09886},
  eprinttype = {arXiv},
  eprint    = {2111.09886},
  timestamp = {Thu, 02 Dec 2021 15:54:22 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-09886.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{attention-is-all-you-need,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  url       = {http://arxiv.org/abs/1706.03762},
  eprinttype = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{t-can-do-bayesian-inference,
  title={Transformers Can Do Bayesian Inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  journal={International Conference on Learning Representations (ICLR) 2022},
  year={2022},
  url        = {https://arxiv.org/abs/2112.10510},
  eprinttype = {arXiv},
  eprint     = {2112.10510},
  timestamp  = {Tue, 04 Jan 2022 15:59:27 +0100},
}


@misc{order-matters,
  doi = {10.48550/ARXIV.1511.06391},

  url = {https://arxiv.org/abs/1511.06391},

  author = {Vinyals, Oriol and Bengio, Samy and Kudlur, Manjunath},

  keywords = {Machine Learning (stat.ML), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Order Matters: Sequence to sequence for sets},

  publisher = {arXiv},

  year = {2015},

  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{order-matters,
  doi = {10.48550/ARXIV.1511.06391},

  url = {https://arxiv.org/abs/1511.06391},

  author = {Vinyals, Oriol and Bengio, Samy and Kudlur, Manjunath},

  keywords = {Machine Learning (stat.ML), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Order Matters: Sequence to sequence for sets},

  publisher = {arXiv},

  year = {2015},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@Article{         numpy,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}


@misc{tensorflow,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@inproceedings{bert,
title	= {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
author	= {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina N. Toutanova},
year	= {2018},
URL	= {https://arxiv.org/abs/1810.04805}
}

@article{wav2vec,
  author    = {Alexei Baevski and
               Henry Zhou and
               Abdelrahman Mohamed and
               Michael Auli},
  title     = {wav2vec 2.0: {A} Framework for Self-Supervised Learning of Speech
               Representations},
  journal   = {CoRR},
  volume    = {abs/2006.11477},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.11477},
  eprinttype = {arXiv},
  eprint    = {2006.11477},
  timestamp = {Tue, 23 Jun 2020 17:57:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-11477.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  url = {https://github.com/openai/gpt-2},
}

@article{gpt3,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  journal   = {CoRR},
  volume    = {abs/2005.14165},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.14165},
  eprinttype = {arXiv},
  eprint    = {2005.14165},
  timestamp = {Wed, 03 Jun 2020 11:36:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{vqgan,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12873--12883},
  year={2021},
  url = {http://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf},
}


@misc{parti,
  doi = {10.48550/ARXIV.2206.10789},

  url = {https://arxiv.org/abs/2206.10789},

  author = {Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and Hutchinson, Ben and Han, Wei and Parekh, Zarana and Li, Xin and Zhang, Han and Baldridge, Jason and Wu, Yonghui},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},

  publisher = {Google},

  year = {2022},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{bart,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7871--7880},
  year={2020},
  url       = {http://arxiv.org/abs/1910.13461},
  eprinttype = {arXiv},
  eprint    = {1910.13461},
  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cross-lingual-mlm,
  title={Cross-lingual language model pretraining},
  author={Conneau, Alexis and Lample, Guillaume},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019},
  url       = {http://arxiv.org/abs/1901.07291},
  eprinttype = {arXiv},
  eprint    = {1901.07291},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-07291.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{unilm,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  url       = {http://arxiv.org/abs/1905.03197},
  eprinttype = {arXiv},
  eprint    = {1905.03197},
}


@inproceedings{prob-masked-lm,
    title = "Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order",
    author = "Liao, Yi  and
      Jiang, Xin  and
      Liu, Qun",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.24",
    doi = "10.18653/v1/2020.acl-main.24",
    pages = "263--274",
    abstract = "Masked language model and autoregressive language model are two types of language models. While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG). In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM). We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive permutated language model. One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation. Besides, the pretrained u-PMLM also outperforms BERT on a bunch of downstream NLU tasks.",
}


@article{multi-query-attn,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019},
  url       = {http://arxiv.org/abs/1911.02150},
  eprinttype = {arXiv},
  eprint    = {1911.02150},
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016},
  url       = {http://arxiv.org/abs/1512.03385},
  eprinttype = {arXiv},
  eprint    = {1512.03385},
}

@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}


@article{transformer-circuits,
  title={A mathematical framework for transformer circuits},
  author={Elhage, N and Nanda, N and Olsson, C and Henighan, T and Joseph, N and Mann, B and Askell, A and Bai, Y and Chen, A and Conerly, T and others},
  journal={Transformer Circuits Thread},
  year={2021},
  url={https://transformer-circuits.pub/2021/framework/index.html}
}

@inbook{von-mises,
  author = {Forbes, Catherine and Evans, Merran and Hastings, Nicholas, and Peacock, Brian},
  pages          = {191},
  publisher      = { Wiley },
  title          = {Statistical Distributions, 4th Edition},
  year           = {2010},
  url           = {http://personalpages.to.infn.it/~zaninett/pdf/statistical-distributions.pdf},
}
@inbook{gaussian,
  author = {Forbes, Catherine and Evans, Merran and Hastings, Nicholas, and Peacock, Brian},
  pages          = {143},
  publisher      = { Wiley },
  title          = {Statistical Distributions, 4th Edition},
  year           = {2010},
  url           = {http://personalpages.to.infn.it/~zaninett/pdf/statistical-distributions.pdf},
}


@techreport{real-time-hand-modeling,
  title={Real-Time Generative Hand Modeling and Tracking},
  author={Tkach, Anastasia},
  year={2018},
  institution={EPFL},
  url={https://infoscience.epfl.ch/record/256674/files/EPFL_TH8573.pdf}
}


@inproceedings{hand-constraints,
  title={Modeling the constraints of human hand motion},
  author={Lin, John and Wu, Ying and Huang, Thomas S},
  booktitle={Proceedings workshop on human motion},
  pages={121--126},
  year={2000},
  organization={IEEE},
  url={http://www.ifp.illinois.edu/~yingwu/papers/Humo00.pdf},
}

@article{human-motion-modeling-survey,
  title={Human motion modeling with deep learning: A survey},
  author={Ye, Zijie and Wu, Haozhe and Jia, Jia},
  journal={AI Open},
  year={2021},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S2666651021000309/pdfft}
}

@inproceedings{dance-choreonet,
  title={Choreonet: Towards music to dance synthesis with choreographic action unit},
  author={Ye, Zijie and Wu, Haozhe and Jia, Jia and Bu, Yaohua and Chen, Wei and Meng, Fanbo and Wang, Yanfeng},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
  pages={744--752},
  year={2020},
  url={https://dl.acm.org/doi/pdf/10.1145/3394171.3414005}
}


@article{dance-transflower,
  title={Transflower: probabilistic autoregressive dance generation with multimodal attention},
  author={Valle-P{\'e}rez, Guillermo and Henter, Gustav Eje and Beskow, Jonas and Holzapfel, Andre and Oudeyer, Pierre-Yves and Alexanderson, Simon},
  journal={ACM Transactions on Graphics (TOG)},
  volume={40},
  number={6},
  pages={1--14},
  year={2021},
  publisher={ACM New York, NY, USA},
  url={https://dl.acm.org/doi/pdf/10.1145/3478513.3480570},
}

@article{hand-pose-survey,
  title={Hand pose estimation: A survey},
  author={Doosti, Bardia},
  journal={arXiv preprint arXiv:1903.01013},
  year={2019},
  url={https://arxiv.org/abs/1903.01013},
}


@inproceedings{structured-prediction,
  title={Structured prediction helps 3d human motion modelling},
  author={Aksan, Emre and Kaufmann, Manuel and Hilliges, Otmar},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7144--7153},
  year={2019},
  url={https://arxiv.org/abs/1910.09070},
}


@InProceedings{signbert,
    author    = {Hu, Hezhen and Zhao, Weichao and Zhou, Wengang and Wang, Yuechen and Li, Houqiang},
    title     = {SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {11087-11096},
    url={https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_SignBERT_Pre-Training_of_Hand-Model-Aware_Representation_for_Sign_Language_Recognition_ICCV_2021_paper.pdf}
}

@article{deep-labeling,
  title={Online optical marker-based hand tracking with deep labels},
  author={Han, Shangchen and Liu, Beibei and Wang, Robert and Ye, Yuting and Twigg, Christopher D and Kin, Kenrick},
  journal={ACM Transactions on Graphics (TOG)},
  volume={37},
  number={4},
  pages={1--10},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@misc{optitrack,
  author       = {NaturalPoint, Inc.},
  howpublished = {Website},
  title        = {OptiTrack},
  year         = {2022},
  url={https://optitrack.com/}
}

@article{transformer-neural-processes,
  title={Transformer neural processes: Uncertainty-aware meta learning via sequence modeling},
  author={Nguyen, Tung and Grover, Aditya},
  journal={arXiv preprint arXiv:2207.04179},
  year={2022},
  url={https://arxiv.org/abs/2207.04179}
}

@article{neural-processes,
  title={Neural processes},
  author={Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J and Eslami, SM and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1807.01622},
  year={2018},
  url={https://arxiv.org/abs/1807.01622},
}

@article{attentive-neural-processes,
  title={Attentive neural processes},
  author={Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1901.05761},
  year={2019},
  url={https://arxiv.org/abs/1901.05761},
}

@article{transformers-bayesian,
  title={Transformers Can Do Bayesian Inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  journal={arXiv preprint arXiv:2112.10510},
  year={2021}
}

@article{alexnet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={AcM New York, NY, USA}
}
