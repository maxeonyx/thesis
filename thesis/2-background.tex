\chapter{Background}
\label{C:background}


\section{Neural Networks and Deep Learning}

Since Krizhevsky et al.'s AlexNet in 2012, many problems are increasingly being solved by the paradigm of Artificial Neural Networks.

Any particular ANN is not designed but discovered by gradient descent, where the parameters of the ANN are iteratively improved with respect to a loss function, which is typically defined with respect to some dataset.

Although many additional variants exist, neural networks can largely be summarized as the application of linear operators and pointwise non-linear operators on tensors of floating point numbers.

\subsection{Notation}
\label{ss:dl-notation}

\TODO{ Differentiate between multi-dimensional data (represented by vectors) are multi-dimensional representations of the data (represented by N-D Tensors. Define batch, sequence, and feature dimensions }

Throughout this thesis I will describe aspects various neural networks with mathematical notation. In order to clarify the notation, I provide an overview here. Firstly, common functions that we see used in neural networks. I use the symbol $\phi$ for activation functions.

Activation functions such as $\relu$ and $\gelu$ are scalar functions, but are typically applied independently across all components of a tensor. To represent such, I will use the following notation:
\begin{align*}
\label{notation:relu, notation:gelu}
    \text{ReLU} & & \fdef{\relu}{\R}{\R} \\
    & & \relu&(x)_i ≝ \max(0,x_i) \\
    \\
    \text{GeLU} & & \fdef{\gelu}{\R}{\R} \\
    & & \gelu&(\x)_i ≝ \x_i \cdot P( \X <= \x_i ) = \x_i \cdot \frac{1}{2}\left(1 + \erf(\x_i/\sqrt{2})\right) \\
    &\text{where} & \erf&(z) = \frac{2}{\sqrt{\pi}}\int^z_0{e^{-t^2}dt}
\end{align*}
In the above equation, the subscript $i$ shows that these functions apply the operation \textit{independently} to each component of the vector $\x$. Below we see how this works for \textit{softmax} which is a vector valued function:
\begin{equation}
\label{notation:softmax}
\begin{split}
    \nvfdef{\sigma}{A×B}{A×B} \\
    \sigma(\X)_{ab} ≝ \frac{e^{\X_{ab}}}{\sum_{b'} e^{\X_{ab'}}}
\end{split}
\end{equation}
The \textit{softmax} function does not apply the operation independently to each component of the vector, but rather the resulting value for each component is dependent on the other components of the vector

Secondly, notation for neural networks. Let $\x \in \R^{N}$ be some input data embedded into an $N$-dimensional vector space. Let $W \in \R^{N \times M} $ be a matrix of learned weights, and let $\phi \colon \R \to \R$ be some non-linear function. Then,
\begin{equation}
\label{notation:mlp}
    f \colon \R^N \to \R^M \\
    f(\x)_{\text{mlp}} ≝ \phi(W\x) + \boldsymbol{b} \\
    W \in \R^{N \times H}, \quad \vb \in \R^H
\end{equation}
represents the computation done by one layer of a simple fully-connected neural network.

A simple classifier network would be defined as follows, for $N$ dimensional data classified into $C$ classes, with $L$ hidden layers:
\begin{align*}
    \vfdef{f_0}{N}{H} \\
    f_{0}&(\x) = \phi(W_0 \x) + \vb &
    W_0 &\in \R^{N\times H} &
    \vb_0 &\in \R^{H}
\\%
    \vfdef{f_l}{H}{H},\ l \in 1,\dots,L \quad \\
    f_l&(\x) = \phi(W_l f_{l-1}(\x)) + \vb_l &
    W_l &\in \R^{H\times H} \quad &
    \vb_l &\in \R^{H}
\\%
    \vfdef{f_L}{H}{C} \\
    f_{L}&(\x) = \sigma(W_L f_{L-1}(\x)) &
    W_L &\in \R^{H \times C}
\\%
    \vfdef{f}{N}{C} \\
    f& = f_L \circ f_{L-1} \circ \cdots \circ f_0 &
    \theta &= \rlap{\{$W_0, \cdots, W_L, \vb_0, \cdots, \vb_{L-1} \} $}
\end{align*}


\subsection{Tasks}

When designing a neural network we can make different choices about the architecture, loss function etc. so that we can use the resulting model on specific tasks:
\begin{itemize}
    \item Training objective:
    \begin{itemize}
        \item Regression (\textit{implicit} MLE)
        \item Parameter estimation (\textit{explicit} MLE)
        \item Classification (Parameter estimation on categorical distributions)
    \end{itemize}
    \item Architecture
    \begin{itemize}
        \item Fixed input size models (CNNs, ResNets, MLPs)
        \item Sequence models (RNNs, Transformers)
    \end{itemize}
    \item Training data design
\end{itemize}
\TODO{ Use the correct terminology for " Regression (\textit{implicit} MLE)" and "Parameter regression (\textit{explicit} MLE)"}

The choice of training objective affects the settings in which a model can be used, which theoretical properties we get from it, and more.

The simplest kind of training objective is regression. When we train a model with a regression objective it learns to predict the expected value of the output. Regression is characterized by using an error function as the loss, for example \textit{mean-squared-error}:
\newcommand{\mse}{L_{\text{MSE}}}
\begin{align}
\label{notation:mse}
\begin{split}
    \fdef{\mse}{\R^{N×D}×\R^{N×D}}{\R} \\
    \mse&(y, \hat{y})_{ni} ≝ \frac{1}{N} \sum_n\left[ \sum_i (y_{ni} - \hat{y}_{ni})^2 \right]
\end{split}
\end{align}
This function sums the error over the \textit{feature} dimension $D$ and averages the error over the \textit{batch} dimension $N$ \footnote{Averaging has no effect on the optimization, it is simply that dividing by the batch and/or sequence length means that the loss value remains in the same range independent of the batch size or sequence length.}. Training a model by minimising this loss function, is equivalent to maximising the likelihood of a Gaussian distribution. Given input $x$, the model output $y = f(x)$ can be interpreted as $\E[p(y|x)]$.


\section{Auto-regressive models}

\TODO{ Formulation of auto-regressive models and sampling }

\TODO{ Things we can do with an auto-regressive model }

\section{Animation}


\section{Angle representations}

\TODO{ Quaternions and unitary / complex matrices }
