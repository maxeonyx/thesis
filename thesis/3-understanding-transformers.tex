\chapter{Understanding Transformers}
\label{C:transformers}

Many of the recent amazing results in deep learning have been achieved with a class of neural networks called \textit{transformers}, which were introduced and named in \cite{attention-is-all-you-need}. In this chapter I seek to understand this class of models at a broader level. I will discuss:

\begin{itemize}
    \item The unique properties of the transformer architecture, which include working with sequences of any length without changing the weights, being able to be computed in parallel across the sequence during training, and being invariant to the order of the inputs.
    \item The different variants of transformers, namely encoder-only, decoder-only, and encoder-decoder models.
    \item The variety of different tasks that this architecture is suited for, which include an extremely generic gaussian-process-like category of tasks.
\end{itemize}

Firstly, I will discuss the defining feature of a transformer which is including at least one \textit{attention} operation.

\section{The Attention Operation}

Attention is a biologically-inspired mechanism that allows a model to recieve inputs from distant parts of the input data, as weighted by the \textit{attention} given to those parts, which is computed from the data itself. This has proven extremely useful for diverse tasks including machine translation, image generation, and more. In this section I will describe the attention operator.

Attention has a number of useful properties which come from its mathematical construction.

\subsection{Mathematical Definition}

An attention operation is of the following form, using short summation notation, where $\sigma$ is the \textit{softmax} operator, and $A$ is the pre-softmax attention logits.
\vspace{-10pt}
\begin{gather*}
    \nfdef{f_{\text{attn}}}{\R^{M×D}×\R^{N×D}×\R^{N×V}}{\R^{M×V}}
\end{gather*}
\vspace{-10pt}
\begin{equation}
\label{eqn:attn}
\begin{split}
    f_{\text{attn}}(Q, K, V)_{mv} ≝ \sum_n \left[\sigma\Big(\sum_d Q_{md} K_{nd}\Big) _{mn} V_{nv} \right]
\end{split}
\end{equation}%
\begin{gather*}
    Q ∈ \R^{M×D}, K ∈ \R^{N×D}, V ∈ \R^{N×V} \\
    M, N, D, V ∈ \N
\end{gather*}\vspace{-10pt}\\
The innermost multiplication of $Q$ and $K$ is simply the inner product (dot product) between vectors $Q_m$ and $K_n$. This however is not inherent. Instead of the inner product, we can substitute any kernel function. (Although this is not usually done because the inner product is the most natural choice, and is efficient to compute)

For clarity, the expanded form of this multiplication, resulting in the unnormalized attention weights $\mathbf{A}$, for an arbitrary kernel function $k$, is as follows:
\begin{align*}
\mathbf{A} = Q K^T &= \begin{bmatrix}
    Q_{1,1} & Q_{1,2} & \cdots & Q_D \\
    Q_{2,1} & Q_{2,2} & \cdots & Q_D \\
    \vdots & \vdots & \ddots & \vdots \\
    Q_{M,1} & Q_{M,2} & \cdots & Q_D
\end{bmatrix} \begin{bmatrix}
    K_{1,1} & K_{1,2} & \cdots & K_D \\
    K_{2,1} & K_{2,2} & \cdots & K_D \\
    \vdots & \vdots & \ddots & \vdots \\
    K_{N,1} & K_{N,2} & \cdots & K_D
\end{bmatrix}^T \\
&= \begin{bmatrix}
    k(Q_{1}, K_{1}) & k(Q_{1}, K_{2}) & \cdots & k(Q_{1}, K_{N}) \\
    k(Q_{2}, K_{1}) & k(Q_{2}, K_{2}) & \cdots & k(Q_{2}, K_{N}) \\
    \vdots & \vdots & \ddots & \vdots \\
    k(Q_{M}, K_{1}) & k(Q_{M}, K_{2}) & \cdots & k(Q_{M}, K_{N})
\end{bmatrix}
\end{align*}
where typically $k = \cdot$ (the dot-product operator).


\subsection{Attention operates on sets}

When processing inputs, an interesting property to have that is useful for some tasks is the ability to operate on sets of inputs. For example, in the case of graphs, or sets of heterogeneous values, there may not be a natural ordering in which to process the inputs. In this case, the attention operator can be used to operate on the entire set of inputs at once. (However, when \textit{sampling} outputs, we typically still need to decide on some order. I will discuss this in more detail in \Cref{C:a-o-sampling}).

Due to the construction of the attention operator, we can see that the output $O_i$ corresponding to a query vector $Q_i$ is independent of the order of the key and value vectors $K_i$ and $V_i$. This is because the computation only involves
\begin{itemize}
    \item pairwise-dot products between $Q_i$ and $K_j$
    \item the softmax operation, which is \textit{equivariant} to the order of its inputs
    \item addition of the corresponding $V_j$ vectors, which is \textit{commutative}
\end{itemize}

The computation of a query vector $Q$ is \textit{invariant} to permutations of the key and value vectors. This is because the softmax operator is \textit{invariant} to permutations of its inputs.

The computations of the query vectors are also independent of each other, which makes the attention operator also \textit{equivariant} to permutations of the query vectors.

This stands in contrast to the convolution operator, which is equivariant to the order of its inputs, but not invariant to permutations of its inputs.

It also stands in contrast to recurrent neural networks, which are neither equivariant nor invariant to permutations of their inputs.

This is a very important property of attention, and I claim it is as yet underappreciated.

\TODO{ Make sure permutation-equivariance is the right terminology }

\TODO{ Explain and justify permutation-invariance of standard transformer layers }


\subsection{Attention variants}

Attention is computed from the three matrices (or sequences of vectors) $Q$, $K$ and $V$. In a neural network, these are each typically derived in some fashion from the inputs to the network. The most common way to do this is to use a learned linear transformation, which is simply a matrix multiplication followed by a bias term, for example

\begin{align}
\label{eqn:attn-linear}
Q &= W_{Q} \X + \vb_{Q} \\
K &= W_{K} \X + \vb_{K} \\
V &= W_{V} \X + \vb_{V}
\end{align}

If we derive all three matrices from the same input $\X$, then $M = N$ and the attention operation is called a \textit{self-attention} operation. A diagram of this is shown in \Cref{fig:self-attn}. The blue shaded areas show the receptive field used when computing each output vector $x'_i$.

When Q and K are derived from separate sequences of feature/embedding vectors, then in general $M ≠ N$ and this is called \textit{cross-attention}. A diagram of this is shown in \Cref{fig:cross-attention}.

\input{thesis/attn-figures.tex}

Since the introduction of transformers it is common to use \textit{multi-head} attention, which allows for multiple \textit{heads} which each perform an attention operation in parallel with smaller key dimensionality $D_{\text{head}} = \frac{D}{ n_{\text{heads}}}$.


\section{Transformer architecture variants}

The defining feature of a transformer model is that it has ``attention'' layers. However, there is not just one way to assemble these layers, and there is not just one way to train these models. Here I discuss a few of the notable variants and the motivation behind them.

\subsection{Masked Sequence Modeling: Encoder-only models}
\label{ss:msm}

Arguably the simplest attention-based model architecture are encoder-only models. These are models trained on a sequence reconstruction task. Examples are the BERT \cite{bert} language model family, Wav2Vec \cite{wav2vec} for speech, and SimMIM \cite{sim-mim} image model.

These models are typically used for sequence understanding tasks and classification tasks. The limitation of these kinds of models is that they  trained in a regression (MAP) setting with respect to sequences of data, or

\subsection{Causal Masking: Decoder-only models}
\label{ss:decoder-only}

\TODO{ Figure of decoder-only model / next-token prediction. }

Currently the most common transformer architecture is the decoder-only architecture, a diagram of which I show below in \TODO{Figure}. These models are used for sequence prediction/generation, and trained via self-supervised learning. The distinguishing feature of a decoder-only model is that its only attention layers are self-attention layers which have a causal mask applied during training.

Some examples of where we see this architecture in use are:
\begin{itemize}
    \item OpenAI's GPT-series \cite{gpt1, gpt2, gpt3} language models.
    \item Latent Sequence Predictors in VQ-GAN \cite{vqgan} and Google's Parti \cite{parti}
\end{itemize}



% transformers first described in "attention is all you need" and have been pushing the state of the art in all domains

% they are a model with a number of useful properties such as permutation invariance (set semantics) and

% there are multiple variations

\subsection {Encoders vs. Decoders}
\label{ss:enc-vs-dec}

As we saw above the encoder differs from the decoder in that the decoder is trained with a causal mask, and the encoder is trained with some other variation such as random masking. When run in inference, they can both be used auto-regressively, but a decoder-only model is more effective at this task, because this is all it was trained to do. The training data for the decoder does not have any other examples in it such as ``fill-in-the-blank'' tasks.

\subsection{Explicit Decoder Targeting}
\label{ss:pure-query}

While it is typical for a decoder to predict the tokens of a sequence in some particular order, we are not limited to this. We can also use the decoder to predict the tokens of a sequence in any order.

This is the approach taken in, for example, ``Order Matters: Sequence to sequence for sets'' \cite{order-matters}.



% Try this paper: "Fast Transformer Decoding: One Write-Head is All You Need"

% https://arxiv.org/pdf/2112.05329.pdf
% FaceFormer: Speech-Driven 3D Facial Animation with Transformers

% - 1 -
% Practical Parameterization of Rotations Using the Exponential Map

%A Spatio-temporal Transformer for 3D Human Motion Prediction

% https://github.com/guillefix/transflower-lightning
% Transflower: Probabilistic autoregressive dance generation with multimodal attention

% https://github.com/maxeonyx/msc-cgt-mnist/blob/5d8787e5f70a0e4c7ef5fcd810891303d700149a/cgt-mnist/entropy-ordering.ipynb
