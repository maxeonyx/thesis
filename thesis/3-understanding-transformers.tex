\chapter{Understanding Transformers}
\label{C:transformers}

Many of the recent amazing results in deep learning have been achieved with a class of neural networks called \textit{transformers}, which were introduced and named in "Attention Is All You Need", Vaswani et al. 2017 \cite{attention-is-all-you-need}. Since then a large number of variants have been developed, and in this chapter I seek to understand this class of models at a broader level. I will discuss:

\begin{itemize}
    \item The unique properties of the transformer architecture, which include working with sequences of any length without changing the weights, being able to be computed in parallel across the sequence during training, and being invariant to the order of the inputs.
    \item The different variants of transformers, namely encoder-only, decoder-only, and encoder-decoder models.
    \item The variety of different tasks that this architecture is suited for, which include an extremely generic gaussian-process-like category of tasks.
\end{itemize}

Firstly, I will discuss the defining feature of a transformer which is including at least one \textit{attention} operation.

\section{The Attention Operation}

Attention is a biologically-inspired mechanism that allows a model to recieve inputs from distant parts of the input data, as weighted by the \textit{attention} given to those parts, which is computed from the data itself. This has proven extremely useful for diverse tasks including machine translation, image generation, and more. In this section I will describe the attention operator.

Attention has a number of useful properties which come from its mathematical construction, such as permutation-invariance in the inputs.

\subsection{Mathematical Definition}

An attention operation is of the following form, using short summation notation, where $\sigma$ is the \textit{softmax} operator (see \ref{eqn:softmax}), and $A$ is the pre-softmax attention logits.
\vspace{-10pt}
\begin{gather*}
    \nfdef{f_{\text{attn}}}{\R^{M×D}×\R^{N×D}×\R^{N×V}}{\R^{M×V}}
\end{gather*}
\vspace{-10pt}
\begin{equation}
\label{eqn:attn}
\begin{split}
    f_{\text{attn}}(Q, K, V)_{mv} ≝ \sum_n \left[\sigma\Big(\sum_d Q_{md} K_{nd}\Big) _{mn} V_{nv} \right]
\end{split}
\end{equation}%
\begin{gather*}
    Q ∈ \R^{M×D}, K ∈ \R^{N×D}, V ∈ \R^{N×V} \\
    M, N, D, V ∈ \N
\end{gather*}\vspace{-10pt}\\
The innermost multiplication of $Q$ and $K$ is simply the inner product (dot product) between vectors $Q_m$ and $K_n$. This however is not inherent. Instead of the inner product, we can substitute any kernel function. (Although this is not usually done because the inner product is the most natural choice, and is efficient to compute)

For clarity, the expanded form of the attention computation, resulting in the unnormalized attention weights $\mathbf{A}$, for an arbitrary kernel function $k$, is as follows:
\begin{align*}
\mathbf{A}_{mn} = k(Q_m, K_n)
&= \begin{bmatrix}
    k(Q_{1}, K_{1}) & k(Q_{1}, K_{2}) & \cdots & k(Q_{1}, K_{N}) \\
    k(Q_{2}, K_{1}) & k(Q_{2}, K_{2}) & \cdots & k(Q_{2}, K_{N}) \\
    \vdots & \vdots & \ddots & \vdots \\
    k(Q_{M}, K_{1}) & k(Q_{M}, K_{2}) & \cdots & k(Q_{M}, K_{N})
\end{bmatrix} \\
\text{or when\ } k(a, b) = a \cdot b &\text{, then} \\
&= \begin{bmatrix}
    Q_{1,1} & Q_{1,2} & \cdots & Q_D \\
    Q_{2,1} & Q_{2,2} & \cdots & Q_D \\
    \vdots & \vdots & \ddots & \vdots \\
    Q_{M,1} & Q_{M,2} & \cdots & Q_D
\end{bmatrix} \begin{bmatrix}
    K_{1,1} & K_{1,2} & \cdots & K_D \\
    K_{2,1} & K_{2,2} & \cdots & K_D \\
    \vdots & \vdots & \ddots & \vdots \\
    K_{N,1} & K_{N,2} & \cdots & K_D
\end{bmatrix}^T \\
&= Q K^T .
\end{align*}

We can see that the attention weights have shape $M×N$. This is the primary drawback of the attention operation, since in self attention $M = N$, and so takes $O(M^2)$ space and time to compute. Despite this drawback, the attention operation has proven extremely useful in a variety of tasks. There are many different ways to address this but I will not discuss them.

\subsection{Permutation-invariance in the context inputs}

The first interesting property of attention is that it is permutation-invariant with respect to the Key-Value inputs. This property is more or less useful depending on the task. For example, in the case of graphs, or sets of heterogeneous values, there may not be a natural ordering in which to process the inputs. In this case, we do not have to introduce any artificial ordering. (However, when \textit{sampling} outputs, we typically still need to decide on some order. I will discuss this in more detail in \Cref{C:a-o-sampling}).

This property is due to the construction of the attention operator. We can see that the output $O_m$ corresponding to a query vector $Q_m$ is independent of the order of the key and value vectors $K_n$ and $V_n$, because the summation across $n$ is commutative:
\begin{align}
\label{eqn:attn-perm-invariance}
\begin{aligned}
    O_m &= \sum V_n \sigma(A_{m})_n
\end{aligned}
\end{align}

\subsection{Permutation-equivariance in the query inputs}

Attention is also permutation-\textit{equivariant} with respect to the Query inputs. This means that the value of the output $O_m$ is dependent on the value of the query vector $Q_m$, but independent of the order of all other query vectors $Q_{m'}$, $m' ≠ m$.
This property is due to the fact that softmax operationk is equivariant to the order of its inputs, which we can see from the construction in \Cref{eqn:softmax}.

This property of attention stands in contrast to the two main other methods used to process sequence data, convolution (CNNs) and recurrence (RNNs). Neither of these operations are invariant (or equivariant) with respect to their inputs.

\subsection{Dynamic length inputs}

The second (and most useful) property of attention is that it can be used to process inputs of dynamic length. We can again see this in \Cref{eqn:attn-perm-invariance}. The softmax operation normalizes the attention weights, which causes the resulting summation of vectors $V_n$ to be a convex combination. The resulting output $O_m$ will therefore sit within the convex hull of the vectors $V_n$. This means that the output $O_m$ will be a ``valid'' output regardless of the length of the input sequence $K_v$.


\subsection{Attention variants}

Attention is computed from the three matrices (or sequences of vectors) $Q$, $K$ and $V$. In a neural network, these are each typically derived in some fashion from the inputs to the network. The most common way to do this is to use a learned linear transformation, which is simply a matrix multiplication followed by a bias term, for example
\begin{align}
\begin{aligned}
\label{eqn:attn-linear}
Q &= W_{Q} \X + \vb_{Q} \\
K &= W_{K} \X + \vb_{K} \\
V &= W_{V} \X + \vb_{V}
\end{aligned}
\end{align}
If we derive all three matrices from the same input $\X$, then $M = N$ and the attention operation is called a \textit{self-attention} operation. A diagram of this is shown in \Cref{fig:self-attn}. The blue shaded areas show the receptive field used when computing each output vector $x'_i$.

When Q and K are derived from separate sequences of feature/embedding vectors, then in general $M ≠ N$ and this is called \textit{cross-attention}. A diagram of this is shown in \Cref{fig:cross-attention}.

\input{thesis/figures-attn.tex}

Since the introduction of transformers it is common to use \textit{multi-head} attention, which allows for multiple \textit{heads} which each perform an attention operation in parallel with smaller key dimensionality $D_{\text{head}} = \frac{D}{ n_{\text{heads}}}$.










\section{Transformer architecture variants}

The defining feature of a transformer model is that it has ``attention'' layers. However, there is not just one way to assemble these layers, and there is not just one way to train these models.

We can broadly split the transformer architecture variants into three categories: \textit{encoder-only}, \textit{decoder-only} and \textit{encoder-decoder} architectures.

\subsection{Masked Sequence Modeling: Encoder-only models}
\label{ss:msm}

Arguably the simplest attention-based model architecture are encoder-only models. These are models trained on a sequence reconstruction task. Examples are the BERT \cite{bert} language model family, Wav2Vec \cite{wav2vec} for speech, and SimMIM \cite{sim-mim} image model.

These models are typically used for sequence understanding tasks and classification tasks. The limitation of these kinds of models is that they  trained in a regression (MAP) setting with respect to sequences of data, or

\subsection{Sequence Prediction: Decoder-only models}
\label{ss:decoder-only}

Currently the most common transformer architecture is the decoder-only architecture, a diagram of which I show below in \Cref{fig:decoder-only}. These models are used for sequence prediction/generation, and trained via self-supervised learning. The distinguishing feature of a decoder-only model is that its only attention layers are self-attention layers which have a causal mask applied during training.

Some examples of where we see this architecture in use are:
\begin{itemize}
    \item OpenAI's GPT-series \cite{gpt2, gpt3} language models.
    \item Latent code prediction (the ``prior'') in VQ-GAN \cite{vqgan}
\end{itemize}

\TODO{ Figure of full decoder-only model / next-token prediction. }

\subsection{Encoder-decoder models}

When the transformer was introduced in \cite{attention-is-all-you-need}, the first architecture proposed was an encoder-decoder architecture. This is a model which has both an encoder and a decoder. The encoder is used to encode a sequence of \textit{conditioning} or \textit{context} inputs, and the decoder is used to generate the output sequence. The encoder and decoder are connected by cross-attention layers (see \Cref{fig:cross-attn}), which allow the decoder to attend to the encoded context sequence.

Examples of this are the original transformer architecture \cite{attention-is-all-you-need}, the BART \cite{bart} model, and more recently Google's Parti multi-modal text-to-image model \cite{parti}.


% transformers first described in "attention is all you need" and have been pushing the state of the art in all domains

% they are a model with a number of useful properties such as permutation invariance (set semantics) and

% there are multiple variations

% \subsection {Encoders vs. Decoders}
% \label{ss:enc-vs-dec}

% As we saw above the encoder differs from the decoder in that the decoder is trained with a causal mask, and the encoder is trained with some other variation such as random masking. When run in inference, they can both be used auto-regressively, but a decoder-only model is more effective at this task, because this is all it was trained to do. The training data for the decoder does not have any other examples in it such as ``fill-in-the-blank'' tasks.

% \subsection{Explicit Decoder Targeting}
% \label{ss:pure-query}

% While it is typical for a decoder to predict the tokens of a sequence in some particular order, we are not limited to this. We can also use the decoder to predict the tokens of a sequence in any order.

% This is the approach taken in, for example, ``Order Matters: Sequence to sequence for sets'' \cite{order-matters}.



% Try this paper: "Fast Transformer Decoding: One Write-Head is All You Need"

% https://arxiv.org/pdf/2112.05329.pdf
% FaceFormer: Speech-Driven 3D Facial Animation with Transformers

% - 1 -
% Practical Parameterization of Rotations Using the Exponential Map

%A Spatio-temporal Transformer for 3D Human Motion Prediction

% https://github.com/guillefix/transflower-lightning
% Transflower: Probabilistic autoregressive dance generation with multimodal attention

% https://github.com/maxeonyx/msc-cgt-mnist/blob/5d8787e5f70a0e4c7ef5fcd810891303d700149a/cgt-mnist/entropy-ordering.ipynb
