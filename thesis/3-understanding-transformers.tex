\chapter{Understanding Transformers}
\label{C:transformers}

Many of the recent amazing results in deep learning have been achieved with transformer models. In this chapter I seek to understand these models at a deeper level,

\TODO{ Point forward to where the various transformer concepts are used. eg. \ref{s:order-tasks} }

\subsection{Attention operator}

An attention operation is of the following form, using short summation notation, where $\sigma$ is the \textit{softmax} operator, and $A$ is the pre-softmax attention logits.
\begin{equation}
\label{eqn:attn}
\begin{split}
    \nfdef{f_{\text{attn}}}{\R^{M×D}×\R^{N×D}×\R^{N×V}}{\R^{M×V}} \\
    f_{\text{attn}}(\mathbf{Q}, \mathbf{K}, \mathbf{V})_{mv} ≝ \sum_n \left[\sigma\Big(\sum_d \mathbf{Q}_{md} \mathbf{K}_{nd}\Big) _{mn} \mathbf{V}_{nv} \right]
\end{split}
\end{equation}
\begin{gather*}
    M, N, D, V ∈ \N \\
    \mathbf{Q} ∈ \R^{M×D}, \mathbf{K} ∈ \R^{N×D}, \mathbf{V} ∈ \R^{N×V}
\end{gather*}

Since the introduction of transformers it is common to use \textit{multi-head} attention, which allows for multiple \textit{heads} which each perform an attention operation in parallel with smaller key dimensionality $D_{\text{head}} = \frac{D}{ n_{\text{heads}}}$.

When we use Q and K derived from the same sequence of feature/embedding vectors, then $M = N$ and it is called \textit{self-attention}.

When Q and K are derived from separate sequences of feature/embedding vectors, then $M ≠_n N$ and this is called \textit{cross-attention}.

A comparison

\subsection{Attention operates on sets}

When processing inputs, an interesting property to have that is useful for some tasks is the ability to operate on sets of inputs. For example, in the case of graphs, or sets of heterogeneous values, there may not be a natural ordering in which to process the inputs. In this case, the attention operator can be used to operate on the entire set of inputs at once. (However, when \textit{sampling} outputs, we typically still need to decide on some order. I will discuss this in more detail in \Cref{C:a-o-sampling}).

Due to the construction of the attention operator, we can see that the output $O_i$ corresponding to a query vector $Q_i$ is independent of the order of the key and value vectors $K_i$ and $V_i$. This is because the computation only involves
\begin{itemize}
    \item pairwise-dot products between $Q_i$ and $K_j$
    \item the softmax operation, which is \textit{equivariant} to the order of its inputs
    \item addition of the corresponding $V_j$ vectors, which is \textit{commutative}
\end{itemize}

The computation of a query vector $Q$ is \textit{invariant} to permutations of the key and value vectors. This is because the softmax operator is \textit{invariant} to permutations of its inputs.

The computations of the query vectors are also independent of each other, which makes the attention operator also \textit{equivariant} to permutations of the query vectors.

This stands in contrast to the convolution operator, which is equivariant to the order of its inputs, but not invariant to permutations of its inputs.

It also stands in contrast to recurrent neural networks, which are neither equivariant nor invariant to permutations of their inputs.

This is a very important property of attention, and I claim it is as yet underappreciated.

\TODO{ Make sure permutation-equivariance is the right terminology }

\TODO{ Explain and justify permutation-invariance of standard transformer layers }

\section{Transformer architecture variants}

The defining feature of a transformer model is that it has ``attention'' layers. However, there is not just one way to assemble these layers, and there is not just one way to train these models. Here I discuss a few of the notable variants and the motivation behind them.

\subsection{Masked Sequence Modeling: Encoder-only models}
\label{ss:msm}

Arguably the simplest attention-based model architecture are encoder-only models. These are models trained on a sequence reconstruction task. Examples are the BERT \cite{bert} language model family, Wav2Vec \cite{wav2vec} for speech, and SimMIM \cite{sim-mim} image model.

These models are typically used for sequence understanding tasks and classification tasks. The limitation of these kinds of models is that they  trained in a regression (MAP) setting with respect to sequences of data, or

\TODO{ Figure of encoder-only model / masked sequence modeling. }



\subsection{Causal Masking: Decoder-only models}
\label{ss:decoder-only}

\TODO{ Figure of decoder-only model / next-token prediction. }

Currently the most common transformer architecture is the decoder-only architecture, a diagram of which I show below in \TODO{Figure}. These models are used for sequence prediction/generation, and trained via self-supervised learning. The distinguishing feature of a decoder-only model is that its only attention layers are self-attention layers which have a causal mask applied during training.

Some examples of where we see this architecture in use are:
\begin{itemize}
    \item OpenAI's GPT-series \cite{gpt1, gpt2, gpt3} language models.
    \item Latent Sequence Predictors in VQ-GAN \cite{vqgan} and Google's Parti \cite{parti}
\end{itemize}



% transformers first described in "attention is all you need" and have been pushing the state of the art in all domains

% they are a model with a number of useful properties such as permutation invariance (set semantics) and

% there are multiple variations

\subsection {Encoders vs. Decoders}
\label{ss:enc-vs-dec}

As we saw above the encoder differs from the decoder in that the decoder is trained with a causal mask, and the encoder is trained with some other variation such as random masking. When run in inference, they can both be used auto-regressively, but a decoder-only model is more effective at this task, because this is all it was trained to do. The training data for the decoder does not have any other examples in it such as ``fill-in-the-blank'' tasks.

\subsection{Explicit Decoder Targeting}
\label{ss:pure-query}

While it is typical for a decoder to predict the tokens of a sequence in some particular order, we are not limited to this. We can also use the decoder to predict the tokens of a sequence in any order.

This is the approach taken in, for example, ``Order Matters: Sequence to sequence for sets'' \cite{order-matters}.



% Try this paper: "Fast Transformer Decoding: One Write-Head is All You Need"

% https://arxiv.org/pdf/2112.05329.pdf
% FaceFormer: Speech-Driven 3D Facial Animation with Transformers

% - 1 -
% Practical Parameterization of Rotations Using the Exponential Map

%A Spatio-temporal Transformer for 3D Human Motion Prediction

% https://github.com/guillefix/transflower-lightning
% Transflower: Probabilistic autoregressive dance generation with multimodal attention

% https://github.com/maxeonyx/msc-cgt-mnist/blob/5d8787e5f70a0e4c7ef5fcd810891303d700149a/cgt-mnist/entropy-ordering.ipynb
