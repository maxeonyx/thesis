
\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{scope}
        \tikzstyle{every node}=[draw,rectangle,text width=0.7cm, minimum width=1.1cm, minimum height=0.8cm]

        \begin{scope}[shift={(1.1cm * -3, 0)}]

            \foreach \x in {1,...,6} {
                \node (i_\x) at (\x*1.1 cm, 0) {$\X_{\x}$};
            }

            \begin{scope}[shift={(0, -2cm)}]
                \foreach \x in {1,...,6} {
                    \node[transform shape,fill=white] (qkv_\x) at (\x*1.1 cm, 0) {$\Q_{\x}$ $\K_{\x}$ $\V_{\x}$};
                    \draw[->] (i_\x.south) -- ([xshift=-4pt]qkv_\x.north);
                    \draw[->] (i_\x.south) -- (qkv_\x.north);
                    \draw[->] (i_\x.south) -- ([xshift=4pt]qkv_\x.north);
                }
            \end{scope}
            \foreach \x in {1,...,6} {
                \begin{scope}[shift={(0, -5cm)}]
                    \node[transform shape,fill=white] (y_\x) at (\x*1.1 cm, 0) {$\X'_{\x}$};
                \end{scope}
                \begin{scope}[
                every path/.append style={draw=blue,fill=blue,fill opacity=0.1,draw opacity=0.2},
                blend group=darken]
                \fill[blue] (y_\x.north) -- ([xshift=-4pt]qkv_1.south) -- ([xshift=4pt]qkv_6.south) -- cycle;
            \end{scope}
            }
        \end{scope}
    \end{scope}

    \draw[thick,decorate,decoration={calligraphic brace,raise=5pt, amplitude=5pt}] (i_1.north) -- node[above=10pt] {Inputs} (i_6.north);

    \end{tikzpicture}
    \caption{Full self-attention, as used in transformer encoders.}
    \label{fig:self-attn}
\end{figure}


\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{scope}
            \tikzstyle{every node}=[draw,rectangle,text width=0.7cm, minimum width=1.1cm, minimum height=0.8cm]

            \begin{scope}[shift={(1.1cm * -3, 0)}]
                \foreach \x in {1,...,6} {
                    \node (i_\x) at (\x*1.1 cm, 0) {$\X_{\x}$};
                }
                \begin{scope}[shift={(0, -2cm)}]
                    \foreach \x in {1,...,6} {
                        \node[transform shape,fill=white] (qkv_\x) at (\x*1.1 cm, 0) {$\Q_{\x}$ $\K_{\x}$ $\V_{\x}$};
                        \draw[->] (i_\x.south) -- ([xshift=-4pt]qkv_\x.north);
                        \draw[->] (i_\x.south) -- (qkv_\x.north);
                        \draw[->] (i_\x.south) -- ([xshift=4pt]qkv_\x.north);
                    }
                \end{scope}
                \foreach \x in {1,...,6} {
                    \begin{scope}[shift={(0, -5cm)}]
                        \node[transform shape,fill=white] (y_\x) at (\x*1.1 cm, 0) {$\X'_{\x}$};
                    \end{scope}
                    \begin{scope}[
                    every path/.append style={draw=blue,fill=blue,fill opacity=0.1,draw opacity=0.2},
                    blend group=darken]
                    \fill[blue] (y_\x.north) -- ([xshift=-4pt]qkv_1.south) -- ([xshift=4pt]qkv_\x.south) -- cycle;
                \end{scope}
                }
            \end{scope}
        \end{scope}

        \draw[thick,decorate,decoration={calligraphic brace,raise=5pt, amplitude=5pt}] (i_1.north) -- node[above=10pt] {Inputs} (i_6.north);

    \end{tikzpicture}
    \caption{Self-attention with causal masking, as used in transformer decoders during training.}
    \label{fig:self-attn-causal}
\end{figure}


\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{scope}
            \tikzstyle{every node}=[draw,rectangle,text width=0.7cm, minimum width=1.1cm, minimum height=0.8cm]

            \begin{scope}[shift={(1.1cm * -3, 0)}]
                \begin{scope}[shift={(-3.4cm, 0)}]
                    \foreach \x in {1,...,7} {
                        \node (x_\x) at (\x*1.1 cm, 0) {$\X_{\x}$};
                    }
                \end{scope}
                \begin{scope}[shift={(-3.4cm, -2cm)}]
                    \foreach \x in {1,...,7} {
                        \node[transform shape,fill=white] (kv_\x) at (\x*1.1 cm, 0) {$\K_{\x}$ $\V_{\x}$};
                        \draw[->] (x_\x.south) -- ([xshift=-2pt]kv_\x.north);
                        \draw[->] (x_\x.south) -- ([xshift=2pt]kv_\x.north);
                    }
                \end{scope}
                \begin{scope}[shift={(4.5cm, 0)}]
                    \foreach \y in {1,...,4} {
                        \node (y_\y) at (\y*1.1 cm, 0) {$\mathbf{Y}_{\y}$};
                    }
                \end{scope}
                \begin{scope}[shift={(4.5cm, -2cm)}]
                    \foreach \y in {1,...,4} {
                        \node[transform shape,fill=white] (q_\y) at (\y*1.1 cm, 0) {$\Q_{\y}$};
                        \draw[->] (y_\y.south) -- (q_\y.north);
                    }
                \end{scope}
                \begin{scope}[shift={(4.5cm, -5cm)}]
                    \foreach \y in {1,...,4} {
                        \node[transform shape,fill=white] (o_\y) at (\y*1.1 cm, 0) {$\mathbf{Y}'_{\y}$};

                        \begin{scope}[
                        every path/.append style={draw=blue,fill=blue,fill opacity=0.1,draw opacity=0.2},
                        blend group=darken]
                            \fill[blue] (o_\y.north) -- (kv_1.south) -- (kv_7.south) -- cycle;
                        \end{scope}
                    }
                \end{scope}
            \end{scope}
        \end{scope}
        \draw[thick,decorate,decoration={calligraphic brace,raise=5pt, amplitude=5pt}] (x_1.north) -- node[above=10pt] {Context Inputs} (x_7.north);
        \draw[thick,decorate,decoration={calligraphic brace,raise=5pt, amplitude=5pt}] (y_1.north) -- node[above=10pt] {Query Inputs} (y_4.north);

    \end{tikzpicture}
    \caption{Cross-attention, as used during during transformer inference, and during training for some variants.}
    \label{fig:cross-attn}
\end{figure}
